{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3b2e16",
   "metadata": {},
   "source": [
    "# STEP 1 â€” DATA INTEGRATION & STRUCTURAL VALIDATION\n",
    "\n",
    "### Objective:\n",
    "- Load and merge transaction + identity datasets\n",
    "- Validate data integrity and structure\n",
    "- Prepare integrated dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bffe8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9334ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1A: Loading Raw Datasets\n",
      "============================================================\n",
      "Transaction dataset shape: (590540, 394)\n",
      "Identity dataset shape: (144233, 41)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1A: Load Raw Datasets\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1A: Loading Raw Datasets\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_tx = pd.read_csv(\"../data/raw/train_transaction.csv\")\n",
    "train_id = pd.read_csv(\"../data/raw/train_identity.csv\")\n",
    "\n",
    "print(f\"Transaction dataset shape: {train_tx.shape}\")\n",
    "print(f\"Identity dataset shape: {train_id.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4187770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1B: Data Integrity Validation\n",
      "============================================================\n",
      "âœ“ TransactionID is unique in both datasets\n",
      "âœ“ No missing TransactionIDs in train_transaction: True\n",
      "âœ“ No missing TransactionIDs in train_identity: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1B: Validate Data Integrity\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1B: Data Integrity Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check TransactionID uniqueness\n",
    "try:\n",
    "    assert train_tx[\"TransactionID\"].is_unique, \"Transaction IDs not unique in train_transaction!\"\n",
    "    assert train_id[\"TransactionID\"].is_unique, \"Transaction IDs not unique in train_identity!\"\n",
    "    print(\"âœ“ TransactionID is unique in both datasets\")\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ CRITICAL ERROR: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check for missing TransactionIDs\n",
    "print(f\"âœ“ No missing TransactionIDs in train_transaction: {train_tx['TransactionID'].isna().sum() == 0}\")\n",
    "print(f\"âœ“ No missing TransactionIDs in train_identity: {train_id['TransactionID'].isna().sum() == 0}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a8007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1C: Merging Datasets\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (590540, 434)\n",
      "Expected shape: (590540, 434)\n",
      "\n",
      "âœ“ Merge validation passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1C: Merge Datasets\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1C: Merging Datasets\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train = train_tx.merge(\n",
    "    train_id,\n",
    "    on=\"TransactionID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset shape: {train.shape}\")\n",
    "print(f\"Expected shape: ({train_tx.shape[0]}, {train_tx.shape[1] + train_id.shape[1] - 1})\")\n",
    "print()\n",
    "\n",
    "# Validate merge\n",
    "assert train.shape[0] == train_tx.shape[0], \"Row count mismatch after merge!\"\n",
    "assert train.shape[1] == train_tx.shape[1] + train_id.shape[1] - 1, \"Column count mismatch after merge!\"\n",
    "print(\"âœ“ Merge validation passed\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8495ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1D: Target Variable Analysis\n",
      "============================================================\n",
      "Fraud distribution (counts):\n",
      "isFraud\n",
      "0    569877\n",
      "1     20663\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud distribution (proportions):\n",
      "isFraud\n",
      "0    0.96501\n",
      "1    0.03499\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ðŸ“Š Fraud rate: 3.50%\n",
      "   Class imbalance ratio: 1:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1D: Target Variable Analysis\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1D: Target Variable Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Fraud distribution (counts):\")\n",
    "print(train[\"isFraud\"].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"Fraud distribution (proportions):\")\n",
    "fraud_dist = train[\"isFraud\"].value_counts(normalize=True)\n",
    "print(fraud_dist)\n",
    "print()\n",
    "\n",
    "fraud_rate = train[\"isFraud\"].mean()\n",
    "print(f\"ðŸ“Š Fraud rate: {fraud_rate:.2%}\")\n",
    "print(f\"   Class imbalance ratio: 1:{int(1/fraud_rate)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96563b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1E: Identity Coverage Analysis\n",
      "============================================================\n",
      "Total identity features: 40\n",
      "\n",
      "Transactions with NO identity info: 75.58%\n",
      "Transactions with SOME identity info: 24.42%\n",
      "\n",
      "Identity feature completeness (top 10 most complete):\n",
      "id_28    0.238727\n",
      "id_29    0.238727\n",
      "id_11    0.238727\n",
      "id_15    0.238739\n",
      "id_35    0.238739\n",
      "id_36    0.238739\n",
      "id_37    0.238739\n",
      "id_38    0.238739\n",
      "id_12    0.244239\n",
      "id_01    0.244239\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1E: Identity Coverage Analysis\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1E: Identity Coverage Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get identity columns (excluding TransactionID)\n",
    "identity_cols = train_id.columns.drop(\"TransactionID\").tolist()\n",
    "\n",
    "print(f\"Total identity features: {len(identity_cols)}\")\n",
    "print()\n",
    "\n",
    "# Calculate rate of completely missing identity info\n",
    "missing_identity_rate = train[identity_cols].isna().all(axis=1).mean()\n",
    "\n",
    "print(f\"Transactions with NO identity info: {missing_identity_rate:.2%}\")\n",
    "print(f\"Transactions with SOME identity info: {1 - missing_identity_rate:.2%}\")\n",
    "print()\n",
    "\n",
    "# Identity feature completeness\n",
    "identity_completeness = (1 - train[identity_cols].isna().mean()).sort_values()\n",
    "print(\"Identity feature completeness (top 10 most complete):\")\n",
    "print(identity_completeness.tail(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a1e6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1F: Data Quality Summary\n",
      "============================================================\n",
      "Total columns: 434\n",
      "Columns with missing values: 414\n",
      "Total missing values: 115,523,073\n",
      "\n",
      "Top 10 columns with most missing values:\n",
      "id_24    585793\n",
      "id_25    585408\n",
      "id_07    585385\n",
      "id_08    585385\n",
      "id_21    585381\n",
      "id_26    585377\n",
      "id_27    585371\n",
      "id_23    585371\n",
      "id_22    585371\n",
      "dist2    552913\n",
      "dtype: int64\n",
      "\n",
      "Memory usage: 2567.09 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1F: Data Quality Summary\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1F: Data Quality Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Missing values overview\n",
    "missing_summary = train.isna().sum().sort_values(ascending=False)\n",
    "missing_top = missing_summary[missing_summary > 0].head(10)\n",
    "\n",
    "print(f\"Total columns: {train.shape[1]}\")\n",
    "print(f\"Columns with missing values: {(missing_summary > 0).sum()}\")\n",
    "print(f\"Total missing values: {train.isna().sum().sum():,}\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 columns with most missing values:\")\n",
    "print(missing_top)\n",
    "print()\n",
    "\n",
    "# Memory usage\n",
    "memory_mb = train.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4cf061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1G: Column Type Summary\n",
      "============================================================\n",
      "Numeric columns: 403\n",
      "Categorical columns: 31\n",
      "\n",
      "Sample categorical columns:\n",
      "['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1G: Column Type Summary\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1G: Column Type Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample categorical columns:\")\n",
    "print(categorical_cols[:10])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8475e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1H: Saving Merged Dataset\n",
      "============================================================\n",
      "âœ“ Saved to: ../data/raw/train_merged.parquet\n",
      "âœ“ Shape: (590540, 434)\n",
      "âœ“ Size: 2567.09 MB\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Step 1 completed successfully!\n",
      "   Next: Step 2 - Data Audit & Risk Profiling\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1H: Save Merged Dataset\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1H: Saving Merged Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_path = \"../data/raw/train_merged.parquet\"\n",
    "train.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Saved to: {output_path}\")\n",
    "print(f\"âœ“ Shape: {train.shape}\")\n",
    "print(f\"âœ“ Size: {memory_mb:.2f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸŽ¯ Step 1 completed successfully!\")\n",
    "print(\"   Next: Step 2 - Data Audit & Risk Profiling\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
